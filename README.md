ğŸ§  Lip Reading from Silent Videos using Deep Learning

This project focuses on automatic lip reading from silent video clips, converting visual mouth movements into readable text using deep learning techniques.
The system processes video frames, extracts lip regions, learns spatio-temporal patterns, and predicts spoken sentences without using audio.

ğŸš€ Project Highlights

ğŸ¥ Lip reading from silent videos only

ğŸ§  Deep Learning model using CNN + LSTM

ğŸ§¾ CTC Loss for sequence-to-sequence prediction

ğŸ“ Automatic lip region extraction

ğŸ” End-to-end pipeline: video â†’ frames â†’ lips â†’ text

ğŸ§ª Trained and tested on real-world dataset samples

ğŸ› ï¸ Technologies & Tools Used
Programming & Frameworks

Python

TensorFlow / Keras

NumPy

OpenCV

Computer Vision

MediaPipe â€“ Face & lip landmark detection

Frame extraction and preprocessing

Deep Learning Architecture

CNN (Convolutional Neural Network) â€“ spatial feature extraction

LSTM (Long Short-Term Memory) â€“ temporal sequence modeling

CTC Loss (Connectionist Temporal Classification) â€“ alignment-free sequence learning

Dataset

GRID Corpus (speaker-wise video and alignment files)

Dataset not included in the repository due to size constraints.

ğŸ“‚ Project Structure
LipReadingProject/
â”‚
â”œâ”€â”€ model/
â”‚   â”œâ”€â”€ cnn_lstm_model.py
â”‚   â”œâ”€â”€ train.py
â”‚   â””â”€â”€ predict.py
â”‚
â”œâ”€â”€ preprocessing/
â”‚   â”œâ”€â”€ extract_frames.py
â”‚   â”œâ”€â”€ lip_extraction.py
â”‚   â”œâ”€â”€ data_loader.py
â”‚   â””â”€â”€ label_parser.py
â”‚
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ text_encoder.py
â”‚
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt

ğŸ”„ Workflow Overview

Video Input

Silent video clips are taken as input.

Frame Extraction

Each video is split into fixed-length frame sequences.

Lip Region Extraction

MediaPipe is used to detect facial landmarks.

Lip regions are cropped and resized.

Data Preparation

Lip frames are normalized and stacked.

Alignment files (.align) are parsed and encoded.

Model Training

CNN extracts spatial features from each frame.

LSTM learns temporal dependencies.

CTC Loss aligns predictions with variable-length text labels.

Prediction

Given a new silent video, the model predicts the spoken sentence.

ğŸ§ª Example Prediction
python model/predict.py bbaf2n


Output:

Predicted Text: bin blue at two

âš ï¸ Notes

Due to large size, datasets, extracted frames, and trained models are excluded from the repository.

The project is designed to be scalable to multiple speakers and vocabularies.

Training on CPU is slow; GPU is recommended for faster experimentation.

ğŸ“Œ Key Learnings

Practical implementation of sequence learning with CTC

Handling visual-only speech recognition

Efficient preprocessing for video-based deep learning

End-to-end ML project structuring and deployment readiness

ğŸ“ˆ Future Improvements

Add Transformer-based architectures

Improve accuracy with data augmentation

Real-time lip reading support

Web interface for live predictions

ğŸ‘¤ Author

Asish Samiraju
Aspiring ML Engineer | Deep Learning & Computer Vision
ğŸ”— GitHub: https://github.com/Asish8239

â­ Acknowledgements

GRID Corpus Dataset

TensorFlow & MediaPipe Teams

Open-source deep learning community